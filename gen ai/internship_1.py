# -*- coding: utf-8 -*-
"""internship-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EHjkO6HkQy1eQx8euiLaGgDHpOq9MHb9
"""

!pip install datasets

from transformers import GPT2LMHeadModel, GPT2Tokenizer
from datasets import load_dataset # datasets module is now imported

model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

dataset = load_dataset('text', data_files={'train': "/content/dialogs.txt"})

# Add a padding token to the tokenizer
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

!pip install transformers datasets

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the model and tokenizer
model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Set pad_token_id to eos_token_id to avoid warnings
tokenizer.pad_token = tokenizer.eos_token

# Input text
input_text = "Hi, how are you doing?"

# Tokenize input text with padding
inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)

# Generate text
output = model.generate(
    inputs['input_ids'],
    attention_mask=inputs['attention_mask'],  # Include attention mask
    max_length=14,
    num_return_sequences=1,
    pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id
)

# Decode and print the output
print(tokenizer.decode(output[0]))

